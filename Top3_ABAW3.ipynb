{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ea5a9c-07ed-4125-ac71-3c2bfcdc0820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from itertools import chain\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(f\"Torch: {torch.__version__}\")\n",
    "device = 'cuda'\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad77ae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model,Sequential, load_model,model_from_json\n",
    "from tensorflow.keras.applications import mobilenet,mobilenet_v2,densenet,inception_resnet_v2,inception_v3,vgg16,resnet_v2,resnet\n",
    "import efficientnet.tfkeras as enet\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout,GlobalAveragePooling2D,Activation, Conv2D, Reshape,DepthwiseConv2D,Input\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, Callback, CSVLogger, EarlyStopping\n",
    "from tensorflow.keras.metrics import top_k_categorical_accuracy\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbc9623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet import preprocess_input\n",
    "from tensorflow.keras.layers import TimeDistributed, GRU, Dense, Dropout, Flatten, LSTM, Activation, MaxPooling2D\n",
    "from tensorflow.keras.regularizers import l2 as L2_reg\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, \\\n",
    "    MaxPool2D, GlobalMaxPool2D, Input, Masking, Conv3D, MaxPooling3D, GlobalMaxPool3D\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "import tensorflow.keras.backend as K \n",
    "\n",
    "print(tf.__version__)\n",
    "from tensorflow.compat.v1.keras.backend import set_session \n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess=tf.compat.v1.Session(config=config)\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b393e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\n",
    "from sklearn import svm,metrics,preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score,roc_auc_score,average_precision_score\n",
    "import mord\n",
    "\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import random\n",
    "import csv  \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a2faca-699d-4cf8-8c7e-d847fdc5460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='enet_b0_8_best_vgaf.pt'\n",
    "IMG_SIZE=224\n",
    "    \n",
    "test_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMG_SIZE,IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(PATH)\n",
    "feature_extractor_model = torch.load('../face-emotion-recognition/models/affectnet_emotions/'+PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8377e1d5-0140-4d5c-8327-4dfd19814cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    classifier_weights=feature_extractor_model.classifier[0].weight.cpu().data.numpy()\n",
    "    classifier_bias=feature_extractor_model.classifier[0].bias.cpu().data.numpy()\n",
    "else:\n",
    "    classifier_weights=feature_extractor_model.classifier.weight.cpu().data.numpy()\n",
    "    classifier_bias=feature_extractor_model.classifier.bias.cpu().data.numpy()\n",
    "print(classifier_weights.shape,classifier_weights)\n",
    "print(classifier_bias.shape,classifier_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0232d3b6-ebbc-459b-aff1-aac4c3af7abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor_model.classifier=torch.nn.Identity()\n",
    "feature_extractor_model=feature_extractor_model.to(device)\n",
    "feature_extractor_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c988148-6b5a-4e79-9669-4f2037cc7c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probab(features, logits=True):\n",
    "    x=np.dot(features,np.transpose(classifier_weights))+classifier_bias\n",
    "    if logits:\n",
    "        return x\n",
    "    e_x = np.exp(x - np.max(x,axis=0))\n",
    "    return e_x / e_x.sum(axis=1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86742670-ee55-4f66-95ed-167fbdc7fb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../../../Data/ABAW4/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d3fed1-9dd0-430f-ac02-0275fd8a3bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_transforms)\n",
    "data_dir=os.path.join(DATA_DIR,'cropped_aligned')\n",
    "#data_dir=os.path.join(DATA_DIR,'cropped_aligned')\n",
    "print(data_dir)\n",
    "img_names=[]\n",
    "X_global_features=[]\n",
    "imgs=[]\n",
    "for filename in tqdm(os.listdir(data_dir)):\n",
    "    frames_dir=os.path.join(data_dir,filename)    \n",
    "    for img_name in os.listdir(frames_dir):\n",
    "        if img_name.lower().endswith('.jpg'):\n",
    "            img = Image.open(os.path.join(frames_dir,img_name))\n",
    "            img_tensor = test_transforms(img)\n",
    "            if img.size:\n",
    "                img_names.append(filename+'/'+img_name)\n",
    "                imgs.append(img_tensor)\n",
    "                if len(imgs)>=96: #48: #64: #32:        \n",
    "                    features = feature_extractor_model(torch.stack(imgs, dim=0).to(device))\n",
    "                    features=features.data.cpu().numpy()\n",
    "                    \n",
    "                    if len(X_global_features)==0:\n",
    "                        X_global_features=features\n",
    "                    else:\n",
    "                        X_global_features=np.concatenate((X_global_features,features),axis=0)\n",
    "                    imgs=[]\n",
    "\n",
    "if len(imgs)>0:        \n",
    "    features = feature_extractor_model(torch.stack(imgs, dim=0).to(device))\n",
    "    features=features.data.cpu().numpy()\n",
    "\n",
    "    if len(X_global_features)==0:\n",
    "        X_global_features=features\n",
    "    else:\n",
    "        X_global_features=np.concatenate((X_global_features,features),axis=0)\n",
    "\n",
    "    imgs=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7ee7c9-f798-49f8-991a-3933ab391d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scores=get_probab(X_global_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e76994-2d14-4945-b5e4-cc093ffb5841",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename2featuresAll={img_name:(global_features,scores) for img_name,global_features,scores in zip(img_names,X_global_features,X_scores)}\n",
    "print(len(filename2featuresAll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a2690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(os.path.join(DATA_DIR, 'enet2_8.pickle'), 'wb') as handle:\n",
    "    pickle.dump(filename2featuresAll, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(DATA_DIR, 'enet2_8.pickle', 'rb') as handle:\n",
    "    filename2featuresAll=pickle.load(handle)\n",
    "print(len(filename2featuresAll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c440863-e54d-4960-b2a8-ed4f2bb3dc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image2all(filename):\n",
    "    with open(os.path.join(DATA_DIR, filename)) as f:\n",
    "        mtl_lines = f.read().splitlines()\n",
    "    num_missed=0\n",
    "    X,y_va,y_expr,y_aus=[],[],[],[]\n",
    "    masks_va,masks_expr,masks_aus=[],[],[]\n",
    "    for line in mtl_lines[1:]:\n",
    "        splitted_line=line.split(',')\n",
    "        imagename=splitted_line[0]\n",
    "        valence=float(splitted_line[1])\n",
    "        arousal=float(splitted_line[2])\n",
    "        expression=int(splitted_line[3])\n",
    "        aus=list(map(int,splitted_line[4:]))\n",
    "        \n",
    "        mask_VA=(valence>-5 and arousal>-5)\n",
    "        if not mask_VA:\n",
    "            valence=arousal=0\n",
    "            \n",
    "        mask_expr=(expression>-1)\n",
    "        if not mask_expr:\n",
    "            expression=0\n",
    "            \n",
    "        mask_aus=min(aus)>=0\n",
    "        if not mask_aus:\n",
    "            aus=[0]*len(aus)\n",
    "        if mask_VA or mask_expr or mask_aus:\n",
    "            if imagename in filename2featuresAll:\n",
    "                #X.append(filename2featuresAll[imagename][0])\n",
    "                X.append(np.concatenate((filename2featuresAll[imagename][0],filename2featuresAll[imagename][1])))\n",
    "                y_va.append((valence,arousal))\n",
    "                masks_va.append(mask_VA)\n",
    "                \n",
    "                y_expr.append(expression)\n",
    "                masks_expr.append(mask_expr)\n",
    "                \n",
    "                y_aus.append(aus)\n",
    "                masks_aus.append(mask_aus)\n",
    "            else:\n",
    "                num_missed+=1\n",
    "    X=np.array(X)\n",
    "    y_va=np.array(y_va)\n",
    "    y_expr=np.array(y_expr)\n",
    "    y_aus=np.array(y_aus)\n",
    "    masks_va=np.array(masks_va).astype(np.float32)\n",
    "    masks_expr=np.array(masks_expr).astype(np.float32)\n",
    "    masks_aus=np.array(masks_aus).astype(np.float32)\n",
    "    print(X.shape,y_va.shape,y_expr.shape,y_aus.shape,masks_va.shape,num_missed)\n",
    "    return X,y_va,y_expr,y_aus,masks_va,masks_expr,masks_aus\n",
    "\n",
    "X_train,y_va_train,y_expr_train,y_aus_train,masks_va_train,masks_expr_train,masks_aus_train=get_image2all('training_set_annotations.txt')\n",
    "X_val,y_va_val,y_expr_val,y_aus_val,masks_va_val,masks_expr_val,masks_aus_val=get_image2all('validation_set_annotations.txt')\n",
    "TRAIN_VAL=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ade41f8-3173-49e9-9c45-2fa3cdd88fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(unique, counts) = np.unique(y_expr_train[masks_expr_train==1.].astype(int), return_counts=True)\n",
    "num_classes=len(unique)\n",
    "emo_cw=1/counts\n",
    "emo_cw/=emo_cw.min()\n",
    "emo_class_weights = {i:cwi for i,cwi in zip(unique,emo_cw)}\n",
    "print(counts, emo_class_weights, num_classes, unique)\n",
    "print(emo_cw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7675c61-bb7c-4fc2-8abf-8d6f46b3ee50",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels=y_aus_train.shape[1]\n",
    "print(num_labels)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "aus_class_weights = np.empty([num_labels, 2])\n",
    "for i in range(num_labels):\n",
    "    neg, pos = np.bincount(y_aus_train[masks_aus_train==1., i])\n",
    "    total = neg + pos\n",
    "    weight_for_0 = (1 / neg) * (total / 2.0)\n",
    "    weight_for_1 = (1 / pos) * (total / 2.0)\n",
    "\n",
    "    aus_class_weights[i][0]=weight_for_0\n",
    "    aus_class_weights[i][1]=weight_for_1\n",
    "print(aus_class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fbb676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CCC_score(x, y):\n",
    "    vx = x - np.mean(x)\n",
    "    vy = y - np.mean(y)\n",
    "    rho = np.sum(vx * vy) / (np.sqrt(np.sum(vx**2)) * np.sqrt(np.sum(vy**2)))\n",
    "    x_m = np.mean(x)\n",
    "    y_m = np.mean(y)\n",
    "    x_s = np.std(x)\n",
    "    y_s = np.std(y)\n",
    "    ccc = 2*rho*x_s*y_s/(x_s**2 + y_s**2 + (x_m - y_m)**2)\n",
    "    return ccc\n",
    "\n",
    "def metric_for_VA(gt_V,gt_A,pred_V,pred_A):\n",
    "    ccc_V,ccc_A = CCC_score(gt_V,pred_V),CCC_score(gt_A,pred_A)\n",
    "    return ccc_V,ccc_A, 0.5*(ccc_V+ccc_A)\n",
    "\n",
    "def CCC_numpy(y_true, y_pred):\n",
    "    '''Reference numpy implementation of Lin's Concordance correlation coefficient'''\n",
    "    \n",
    "    # covariance between y_true and y_pred\n",
    "    s_xy = np.cov([y_true, y_pred])[0,1]\n",
    "    # means\n",
    "    x_m = np.mean(y_true)\n",
    "    y_m = np.mean(y_pred)\n",
    "    # variances\n",
    "    s_x_sq = np.var(y_true)\n",
    "    s_y_sq = np.var(y_pred)\n",
    "    \n",
    "    # condordance correlation coefficient\n",
    "    ccc = (2.0*s_xy) / (s_x_sq + s_y_sq + (x_m-y_m)**2)\n",
    "    \n",
    "    return ccc\n",
    "\n",
    "def CCC(y_true, y_pred):\n",
    "    '''Lin's Concordance correlation coefficient: https://en.wikipedia.org/wiki/Concordance_correlation_coefficient\n",
    "    \n",
    "    The concordance correlation coefficient is the correlation between two variables that fall on the 45 degree line through the origin.\n",
    "    \n",
    "    It is a product of\n",
    "    - precision (Pearson correlation coefficient) and\n",
    "    - accuracy (closeness to 45 degree line)\n",
    "\n",
    "    Interpretation:\n",
    "    - `rho_c =  1` : perfect agreement\n",
    "    - `rho_c =  0` : no agreement\n",
    "    - `rho_c = -1` : perfect disagreement \n",
    "    \n",
    "    Args: \n",
    "    - y_true: ground truth\n",
    "    - y_pred: predicted values\n",
    "    \n",
    "    Returns:\n",
    "    - concordance correlation coefficient (float)\n",
    "    '''\n",
    "    \n",
    "    # covariance between y_true and y_pred\n",
    "    s_xy = K.mean((y_true - K.mean(y_true)) * (y_pred - K.mean(y_pred)))\n",
    "    # means\n",
    "    x_m = K.mean(y_true)\n",
    "    y_m = K.mean(y_pred)\n",
    "    # variances\n",
    "    s_x_sq = K.var(y_true)\n",
    "    s_y_sq = K.var(y_pred)\n",
    "    \n",
    "    # condordance correlation coefficient\n",
    "    ccc = (2.0*s_xy) / (s_x_sq + s_y_sq + (x_m-y_m)**2+K.epsilon())\n",
    "    return ccc\n",
    "\n",
    "def CCC_VA(y_true, y_pred):\n",
    "    return 1-0.5*(CCC(y_true[:,0], y_pred[:,0])+CCC(y_true[:,1], y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbb0402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def f1_score_max_for_AU_one_class(gt, pred, thresh,type=0):\n",
    "    gt = gt[:,type]\n",
    "    pred = pred[:,type]\n",
    "    P = []\n",
    "    R = []\n",
    "    ACC = []\n",
    "    F1 = []\n",
    "    for i in thresh:\n",
    "        new_pred = ((pred >= i) * 1).flatten()\n",
    "        P.append(precision_score(gt.flatten(), new_pred))\n",
    "        R.append(recall_score(gt.flatten(), new_pred))\n",
    "        ACC.append(accuracy_score(gt.flatten(), new_pred))\n",
    "        F1.append(f1_score(gt.flatten(), new_pred))\n",
    "\n",
    "    F1_MAX = max(F1)\n",
    "    if F1_MAX < 0 or math.isnan(F1_MAX):\n",
    "        F1_MAX = 0\n",
    "        F1_THRESH = 0\n",
    "        accuracy = 0\n",
    "    else:\n",
    "        idx_thresh = np.argmax(F1)\n",
    "        F1_THRESH = thresh[idx_thresh]\n",
    "        accuracy = ACC[idx_thresh]\n",
    "    return F1,F1_MAX,F1_THRESH,accuracy\n",
    "\n",
    "def f1_score_max(gt, pred, thresh,c=12):\n",
    "    F1_s = []\n",
    "    F1_t = []\n",
    "    ACC = []\n",
    "    from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "    for i in range(c):\n",
    "        F1, F1_MAX, F1_THRESH,acc = f1_score_max_for_AU_one_class(gt,pred,thresh,i)\n",
    "        F1_s.append(F1_MAX)\n",
    "        F1_t.append(F1_THRESH)\n",
    "        ACC.append(acc)\n",
    "    F1_s=np.array(F1_s)\n",
    "    F1_t=np.array(F1_t)\n",
    "    ACC=np.array(ACC)\n",
    "    return F1_s.mean(),F1_t.mean(),F1_s,F1_t,ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b518196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_transfer(label,class_num):\n",
    "    return np.eye(class_num)[label]\n",
    "\n",
    "def metric_for_Exp(gt,pred,class_num=8):\n",
    "    acc = accuracy_score(gt,pred)\n",
    "    \n",
    "    # compute_F1\n",
    "    gt = one_hot_transfer(gt,class_num)\n",
    "    pred = one_hot_transfer(pred,class_num)\n",
    "    F1 = []\n",
    "    for i in range(class_num):\n",
    "        gt_ = gt[:,i]\n",
    "        pred_ = pred[:,i]\n",
    "        F1.append(f1_score(gt_.flatten(), pred_))\n",
    "    F1_mean = np.mean(F1)\n",
    "    return F1_mean,acc,F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825442d8-adc5-443e-a352-dd8e1d610504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_va(y_true, y_pred):\n",
    "    res=1-0.5*(CCC(y_true[:,0], y_pred[:,0])+CCC(y_true[:,1], y_pred[:,1]))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f00bd3-4b8e-43f0-bebc-31f0acdeecae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedExprSCCE(tf.keras.losses.Loss):\n",
    "    def __init__(self, class_weight, from_logits=False, name='expr_scce'):\n",
    "        if class_weight is None or all(v == 1. for v in class_weight):\n",
    "            self.class_weight = None\n",
    "        else:\n",
    "            self.class_weight = tf.convert_to_tensor(class_weight,\n",
    "                dtype=tf.float32)\n",
    "        self.reduction = tf.keras.losses.Reduction.NONE\n",
    "        self.unreduced_scce = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=from_logits, name=name,\n",
    "            reduction=self.reduction)\n",
    "\n",
    "    def __call__(self, y_true, y_pred, sample_weight=None):\n",
    "        loss = self.unreduced_scce(y_true, y_pred, sample_weight)\n",
    "        if self.class_weight is not None:\n",
    "            weight_mask = tf.gather(self.class_weight, y_true)\n",
    "            loss = tf.math.multiply(loss, weight_mask)\n",
    "        loss=K.mean(loss)\n",
    "        return loss\n",
    "loss_expr=WeightedExprSCCE(emo_cw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2b7ce9-c4a4-4d44-af8f-f7a605ea8e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_loss_aus(weights):\n",
    "    def weighted_loss(y_true, y_pred):\n",
    "        y_true=tf.cast(y_true, tf.float32)\n",
    "        ce=K.binary_crossentropy(y_true, y_pred)\n",
    "        res=K.mean((weights[:,0]**(1-y_true))*(weights[:,1]**(y_true))*ce)\n",
    "        return res\n",
    "    return weighted_loss\n",
    "loss_aus=get_weighted_loss_aus(aus_class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f72c67-ed55-4ff8-8115-fe07983dc972",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=[tf.keras.metrics.AUC(multi_label=True,name='auc'), tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.Recall(),tf.keras.metrics.Precision()] # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ede4004-373c-4767-9df2-c1ca95ac994f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=256 #128\n",
    "img = tf.keras.Input(shape=X_train.shape[1:])\n",
    "mask1 = tf.keras.Input(shape=(1,))\n",
    "mask2 = tf.keras.Input(shape=(1,))\n",
    "mask3 = tf.keras.Input(shape=(1,))\n",
    "x=img\n",
    "#x=Dense(128, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(1.0/batch_size))(x)\n",
    "va_out = Dense(2, activation='tanh',kernel_regularizer=tf.keras.regularizers.l2(1.0/batch_size))(x)\n",
    "va_out_masked=tf.keras.layers.Multiply(name='va_out')([va_out,mask1])\n",
    "\n",
    "#x=va_out\n",
    "\n",
    "expr_out=Dense(8, activation='softmax',kernel_regularizer=tf.keras.regularizers.l2(1.0/batch_size))(x)\n",
    "expr_out_masked=tf.keras.layers.Multiply(name='expr_out')([expr_out,mask2])\n",
    "aus_out=Dense(12, activation='sigmoid',kernel_regularizer=tf.keras.regularizers.l2(1.0/batch_size))(x)\n",
    "aus_out_masked=tf.keras.layers.Multiply(name='aus_out')([aus_out,mask3])\n",
    "\n",
    "mtlModel=tf.keras.Model(inputs=[img,mask1,mask2,mask3], outputs=[va_out_masked, expr_out_masked,aus_out_masked])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed55b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "class SaveBestModel(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, save_best_metric='val_loss', this_max=False):\n",
    "        self.save_best_metric = save_best_metric\n",
    "        self.max = this_max\n",
    "        if this_max:\n",
    "            self.best = float('-inf')\n",
    "        else:\n",
    "            self.best = float('inf')\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        metric_value = logs[self.save_best_metric]\n",
    "        if self.max:\n",
    "            if metric_value > self.best:\n",
    "                self.best = metric_value\n",
    "                self.best_model_weights = deepcopy(self.model.get_weights())\n",
    "\n",
    "        else:\n",
    "            if metric_value < self.best:\n",
    "                self.best = metric_value\n",
    "                self.best_model_weights = deepcopy(self.model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc88bb2-a7bd-4ef6-ab10-82469cb2b283",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtlModel.compile(optimizer=Adam(lr=1e-3), loss=[loss_va,loss_expr,loss_aus], metrics=[[\"mean_absolute_error\"], [\"accuracy\"],metrics])\n",
    "mtlModel.summary()\n",
    "\n",
    "save_best_model = SaveBestModel('val_loss',False)\n",
    "mtlModel.fit([X_train,masks_va_train,masks_expr_train,masks_aus_train],\n",
    "             [y_va_train,y_expr_train,y_aus_train], batch_size=batch_size, epochs=(1 if TRAIN_VAL else 20), \n",
    "             verbose=1, callbacks=[save_best_model], \n",
    "             validation_data=([X_val,masks_va_val,masks_expr_val,masks_aus_val],[y_va_val,y_expr_val,y_aus_val]))\n",
    "best_model_weights = save_best_model.best_model_weights\n",
    "print(save_best_model.best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7b4e8c-f676-4f94-97db-0dab68813b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_all():\n",
    "    y_pred_va,y_pred_expr,y_pred_aus=mtlModel.predict([X_val,masks_va_val,masks_expr_val,masks_aus_val])\n",
    "    print('\\nAV')\n",
    "    gt_V=y_va_val[masks_va_val==1,0]\n",
    "    gt_A=y_va_val[masks_va_val==1,1]\n",
    "    pred_V=y_pred_va[masks_va_val==1,0]\n",
    "    pred_A=y_pred_va[masks_va_val==1,1]\n",
    "    ccc_V,ccc_A,ccc_VA=metric_for_VA(gt_V,gt_A,pred_V,pred_A)\n",
    "    print(gt_V.shape,ccc_V,ccc_A,ccc_VA)\n",
    "    \n",
    "    print('\\nExpression')\n",
    "    print(y_expr_val[masks_expr_val==1].shape)\n",
    "    y_pred=np.argmax(y_pred_expr,axis=1)\n",
    "    print((y_pred==y_expr_val)[masks_expr_val==1].mean())\n",
    "    f1_expr=f1_score(y_true=y_expr_val[masks_expr_val==1],y_pred=y_pred[masks_expr_val==1], average=\"macro\")\n",
    "    print(f1_expr)\n",
    "    print(metric_for_Exp(y_expr_val[masks_expr_val==1],y_pred[masks_expr_val==1]))\n",
    "    \n",
    "    print('\\nAUs')\n",
    "    new_pred = ((y_pred_aus >= 0.5) * 1)\n",
    "    print(new_pred[masks_aus_val==1,:].shape)\n",
    "    f1_au=np.mean([f1_score(y_true=y_aus_val[masks_aus_val==1,i],y_pred=new_pred[masks_aus_val==1,i]) for i in range(y_pred_aus.shape[1])])\n",
    "    print(f1_au)\n",
    "    print(f1_score_max(y_aus_val[masks_aus_val==1,:],y_pred_aus[masks_aus_val==1,:],thresh=np.arange(0.1,1,0.1)))\n",
    "    \n",
    "    total=ccc_VA+f1_expr+f1_au\n",
    "    print('\\nTotal',ccc_VA,f1_expr,f1_au,total)\n",
    "\n",
    "print_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
