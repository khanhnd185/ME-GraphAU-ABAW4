{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ea5a9c-07ed-4125-ac71-3c2bfcdc0820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from itertools import chain\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(f\"Torch: {torch.__version__}\")\n",
    "device = 'cuda'\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a2faca-699d-4cf8-8c7e-d847fdc5460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='enet_b0_8_best_vgaf.pt'\n",
    "IMG_SIZE=224\n",
    "    \n",
    "test_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMG_SIZE,IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(PATH)\n",
    "feature_extractor_model = torch.load('../face-emotion-recognition/models/affectnet_emotions/'+PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8377e1d5-0140-4d5c-8327-4dfd19814cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    classifier_weights=feature_extractor_model.classifier[0].weight.cpu().data.numpy()\n",
    "    classifier_bias=feature_extractor_model.classifier[0].bias.cpu().data.numpy()\n",
    "else:\n",
    "    classifier_weights=feature_extractor_model.classifier.weight.cpu().data.numpy()\n",
    "    classifier_bias=feature_extractor_model.classifier.bias.cpu().data.numpy()\n",
    "print(classifier_weights.shape,classifier_weights)\n",
    "print(classifier_bias.shape,classifier_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0232d3b6-ebbc-459b-aff1-aac4c3af7abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor_model.classifier=torch.nn.Identity()\n",
    "feature_extractor_model=feature_extractor_model.to(device)\n",
    "feature_extractor_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c988148-6b5a-4e79-9669-4f2037cc7c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probab(features, logits=True):\n",
    "    x=np.dot(features,np.transpose(classifier_weights))+classifier_bias\n",
    "    if logits:\n",
    "        return x\n",
    "    e_x = np.exp(x - np.max(x,axis=0))\n",
    "    return e_x / e_x.sum(axis=1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86742670-ee55-4f66-95ed-167fbdc7fb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../../../Data/ABAW4/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d3fed1-9dd0-430f-ac02-0275fd8a3bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_transforms)\n",
    "data_dir=os.path.join(DATA_DIR,'cropped_aligned')\n",
    "#data_dir=os.path.join(DATA_DIR,'cropped_aligned')\n",
    "print(data_dir)\n",
    "img_names=[]\n",
    "X_global_features=[]\n",
    "imgs=[]\n",
    "for filename in tqdm(os.listdir(data_dir)):\n",
    "    frames_dir=os.path.join(data_dir,filename)    \n",
    "    for img_name in os.listdir(frames_dir):\n",
    "        if img_name.lower().endswith('.jpg'):\n",
    "            img = Image.open(os.path.join(frames_dir,img_name))\n",
    "            img_tensor = test_transforms(img)\n",
    "            if img.size:\n",
    "                img_names.append(filename+'/'+img_name)\n",
    "                imgs.append(img_tensor)\n",
    "                if len(imgs)>=96: #48: #64: #32:        \n",
    "                    features = feature_extractor_model(torch.stack(imgs, dim=0).to(device))\n",
    "                    features=features.data.cpu().numpy()\n",
    "                    \n",
    "                    if len(X_global_features)==0:\n",
    "                        X_global_features=features\n",
    "                    else:\n",
    "                        X_global_features=np.concatenate((X_global_features,features),axis=0)\n",
    "                    imgs=[]\n",
    "\n",
    "if len(imgs)>0:        \n",
    "    features = feature_extractor_model(torch.stack(imgs, dim=0).to(device))\n",
    "    features=features.data.cpu().numpy()\n",
    "\n",
    "    if len(X_global_features)==0:\n",
    "        X_global_features=features\n",
    "    else:\n",
    "        X_global_features=np.concatenate((X_global_features,features),axis=0)\n",
    "\n",
    "    imgs=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7ee7c9-f798-49f8-991a-3933ab391d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scores=get_probab(X_global_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e76994-2d14-4945-b5e4-cc093ffb5841",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename2featuresAll={img_name:(global_features,scores) for img_name,global_features,scores in zip(img_names,X_global_features,X_scores)}\n",
    "print(len(filename2featuresAll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c440863-e54d-4960-b2a8-ed4f2bb3dc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image2all(filename):\n",
    "    with open(os.path.join(DATA_DIR, filename)) as f:\n",
    "        mtl_lines = f.read().splitlines()\n",
    "    num_missed=0\n",
    "    X,y_va,y_expr,y_aus=[],[],[],[]\n",
    "    masks_va,masks_expr,masks_aus=[],[],[]\n",
    "    for line in mtl_lines[1:]:\n",
    "        splitted_line=line.split(',')\n",
    "        imagename=splitted_line[0]\n",
    "        valence=float(splitted_line[1])\n",
    "        arousal=float(splitted_line[2])\n",
    "        expression=int(splitted_line[3])\n",
    "        aus=list(map(int,splitted_line[4:]))\n",
    "        \n",
    "        mask_VA=(valence>-5 and arousal>-5)\n",
    "        if not mask_VA:\n",
    "            valence=arousal=0\n",
    "            \n",
    "        mask_expr=(expression>-1)\n",
    "        if not mask_expr:\n",
    "            expression=0\n",
    "            \n",
    "        mask_aus=min(aus)>=0\n",
    "        if not mask_aus:\n",
    "            aus=[0]*len(aus)\n",
    "        if mask_VA or mask_expr or mask_aus:\n",
    "            if imagename in filename2featuresAll:\n",
    "                #X.append(filename2featuresAll[imagename][0])\n",
    "                X.append(np.concatenate((filename2featuresAll[imagename][0],filename2featuresAll[imagename][1])))\n",
    "                y_va.append((valence,arousal))\n",
    "                masks_va.append(mask_VA)\n",
    "                \n",
    "                y_expr.append(expression)\n",
    "                masks_expr.append(mask_expr)\n",
    "                \n",
    "                y_aus.append(aus)\n",
    "                masks_aus.append(mask_aus)\n",
    "            else:\n",
    "                num_missed+=1\n",
    "    X=np.array(X)\n",
    "    y_va=np.array(y_va)\n",
    "    y_expr=np.array(y_expr)\n",
    "    y_aus=np.array(y_aus)\n",
    "    masks_va=np.array(masks_va).astype(np.float32)\n",
    "    masks_expr=np.array(masks_expr).astype(np.float32)\n",
    "    masks_aus=np.array(masks_aus).astype(np.float32)\n",
    "    print(X.shape,y_va.shape,y_expr.shape,y_aus.shape,masks_va.shape,num_missed)\n",
    "    return X,y_va,y_expr,y_aus,masks_va,masks_expr,masks_aus\n",
    "\n",
    "X_train,y_va_train,y_expr_train,y_aus_train,masks_va_train,masks_expr_train,masks_aus_train=get_image2all('training_set_annotations.txt')\n",
    "X_val,y_va_val,y_expr_val,y_aus_val,masks_va_val,masks_expr_val,masks_aus_val=get_image2all('validation_set_annotations.txt')\n",
    "TRAIN_VAL=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ade41f8-3173-49e9-9c45-2fa3cdd88fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(unique, counts) = np.unique(y_expr_train[masks_expr_train==1.].astype(int), return_counts=True)\n",
    "num_classes=len(unique)\n",
    "emo_cw=1/counts\n",
    "emo_cw/=emo_cw.min()\n",
    "emo_class_weights = {i:cwi for i,cwi in zip(unique,emo_cw)}\n",
    "print(counts, emo_class_weights, num_classes, unique)\n",
    "print(emo_cw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7675c61-bb7c-4fc2-8abf-8d6f46b3ee50",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels=y_aus_train.shape[1]\n",
    "print(num_labels)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "aus_class_weights = np.empty([num_labels, 2])\n",
    "for i in range(num_labels):\n",
    "    neg, pos = np.bincount(y_aus_train[masks_aus_train==1., i])\n",
    "    total = neg + pos\n",
    "    weight_for_0 = (1 / neg) * (total / 2.0)\n",
    "    weight_for_1 = (1 / pos) * (total / 2.0)\n",
    "\n",
    "    aus_class_weights[i][0]=weight_for_0\n",
    "    aus_class_weights[i][1]=weight_for_1\n",
    "print(aus_class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825442d8-adc5-443e-a352-dd8e1d610504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_va(y_true, y_pred):\n",
    "    res=1-0.5*(CCC(y_true[:,0], y_pred[:,0])+CCC(y_true[:,1], y_pred[:,1]))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f00bd3-4b8e-43f0-bebc-31f0acdeecae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedExprSCCE(tf.keras.losses.Loss):\n",
    "    def __init__(self, class_weight, from_logits=False, name='expr_scce'):\n",
    "        if class_weight is None or all(v == 1. for v in class_weight):\n",
    "            self.class_weight = None\n",
    "        else:\n",
    "            self.class_weight = tf.convert_to_tensor(class_weight,\n",
    "                dtype=tf.float32)\n",
    "        self.reduction = tf.keras.losses.Reduction.NONE\n",
    "        self.unreduced_scce = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=from_logits, name=name,\n",
    "            reduction=self.reduction)\n",
    "\n",
    "    def __call__(self, y_true, y_pred, sample_weight=None):\n",
    "        loss = self.unreduced_scce(y_true, y_pred, sample_weight)\n",
    "        if self.class_weight is not None:\n",
    "            weight_mask = tf.gather(self.class_weight, y_true)\n",
    "            loss = tf.math.multiply(loss, weight_mask)\n",
    "        loss=K.mean(loss)\n",
    "        return loss\n",
    "loss_expr=WeightedExprSCCE(emo_cw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2b7ce9-c4a4-4d44-af8f-f7a605ea8e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_loss_aus(weights):\n",
    "    def weighted_loss(y_true, y_pred):\n",
    "        y_true=tf.cast(y_true, tf.float32)\n",
    "        ce=K.binary_crossentropy(y_true, y_pred)\n",
    "        res=K.mean((weights[:,0]**(1-y_true))*(weights[:,1]**(y_true))*ce)\n",
    "        return res\n",
    "    return weighted_loss\n",
    "loss_aus=get_weighted_loss_aus(aus_class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f72c67-ed55-4ff8-8115-fe07983dc972",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=[tf.keras.metrics.AUC(multi_label=True,name='auc'), tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.Recall(),tf.keras.metrics.Precision()] # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ede4004-373c-4767-9df2-c1ca95ac994f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=256 #128\n",
    "img = tf.keras.Input(shape=X_train.shape[1:])\n",
    "mask1 = tf.keras.Input(shape=(1,))\n",
    "mask2 = tf.keras.Input(shape=(1,))\n",
    "mask3 = tf.keras.Input(shape=(1,))\n",
    "x=img\n",
    "#x=Dense(128, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(1.0/batch_size))(x)\n",
    "va_out = Dense(2, activation='tanh',kernel_regularizer=tf.keras.regularizers.l2(1.0/batch_size))(x)\n",
    "va_out_masked=tf.keras.layers.Multiply(name='va_out')([va_out,mask1])\n",
    "\n",
    "#x=va_out\n",
    "\n",
    "expr_out=Dense(8, activation='softmax',kernel_regularizer=tf.keras.regularizers.l2(1.0/batch_size))(x)\n",
    "expr_out_masked=tf.keras.layers.Multiply(name='expr_out')([expr_out,mask2])\n",
    "aus_out=Dense(12, activation='sigmoid',kernel_regularizer=tf.keras.regularizers.l2(1.0/batch_size))(x)\n",
    "aus_out_masked=tf.keras.layers.Multiply(name='aus_out')([aus_out,mask3])\n",
    "\n",
    "mtlModel=tf.keras.Model(inputs=[img,mask1,mask2,mask3], outputs=[va_out_masked, expr_out_masked,aus_out_masked])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc88bb2-a7bd-4ef6-ab10-82469cb2b283",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtlModel.compile(optimizer=Adam(lr=1e-3), loss=[loss_va,loss_expr,loss_aus], metrics=[[\"mean_absolute_error\"], [\"accuracy\"],metrics])\n",
    "mtlModel.summary()\n",
    "\n",
    "save_best_model = SaveBestModel('val_loss',False)\n",
    "mtlModel.fit([X_train,masks_va_train,masks_expr_train,masks_aus_train],\n",
    "             [y_va_train,y_expr_train,y_aus_train], batch_size=batch_size, epochs=(1 if TRAIN_VAL else 20), \n",
    "             verbose=1, callbacks=[save_best_model], \n",
    "             validation_data=([X_val,masks_va_val,masks_expr_val,masks_aus_val],[y_va_val,y_expr_val,y_aus_val]))\n",
    "best_model_weights = save_best_model.best_model_weights\n",
    "print(save_best_model.best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7b4e8c-f676-4f94-97db-0dab68813b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_all():\n",
    "    y_pred_va,y_pred_expr,y_pred_aus=mtlModel.predict([X_val,masks_va_val,masks_expr_val,masks_aus_val])\n",
    "    print('\\nAV')\n",
    "    gt_V=y_va_val[masks_va_val==1,0]\n",
    "    gt_A=y_va_val[masks_va_val==1,1]\n",
    "    pred_V=y_pred_va[masks_va_val==1,0]\n",
    "    pred_A=y_pred_va[masks_va_val==1,1]\n",
    "    ccc_V,ccc_A,ccc_VA=metric_for_VA(gt_V,gt_A,pred_V,pred_A)\n",
    "    print(gt_V.shape,ccc_V,ccc_A,ccc_VA)\n",
    "    \n",
    "    print('\\nExpression')\n",
    "    print(y_expr_val[masks_expr_val==1].shape)\n",
    "    y_pred=np.argmax(y_pred_expr,axis=1)\n",
    "    print((y_pred==y_expr_val)[masks_expr_val==1].mean())\n",
    "    f1_expr=f1_score(y_true=y_expr_val[masks_expr_val==1],y_pred=y_pred[masks_expr_val==1], average=\"macro\")\n",
    "    print(f1_expr)\n",
    "    print(metric_for_Exp(y_expr_val[masks_expr_val==1],y_pred[masks_expr_val==1]))\n",
    "    \n",
    "    print('\\nAUs')\n",
    "    new_pred = ((y_pred_aus >= 0.5) * 1)\n",
    "    print(new_pred[masks_aus_val==1,:].shape)\n",
    "    f1_au=np.mean([f1_score(y_true=y_aus_val[masks_aus_val==1,i],y_pred=new_pred[masks_aus_val==1,i]) for i in range(y_pred_aus.shape[1])])\n",
    "    print(f1_au)\n",
    "    print(f1_score_max(y_aus_val[masks_aus_val==1,:],y_pred_aus[masks_aus_val==1,:],thresh=np.arange(0.1,1,0.1)))\n",
    "    \n",
    "    total=ccc_VA+f1_expr+f1_au\n",
    "    print('\\nTotal',ccc_VA,f1_expr,f1_au,total)\n",
    "\n",
    "print_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
